{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-5d741664957b>, line 216)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5d741664957b>\"\u001b[0;36m, line \u001b[0;32m216\u001b[0m\n\u001b[0;31m    identity_list_cur[RB.i_labels.index(p_name)] = 1s\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import RM\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas\n",
    "import ast\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "\n",
    "def resetFiles():\n",
    "    os.remove(RB.recog_file)\n",
    "    os.remove(RB.csv_file)\n",
    "    with open(RB.csv_file, 'wb') as outcsv:\n",
    "        writer = csv.writer(outcsv)\n",
    "        writer.writerow([\"I\", \"F\", \"G\", \"A\", \"H\", \"T\", \"R\"])\n",
    "    \n",
    "    os.remove(RB.initial_recognition_file)\n",
    "    with open(RB.initial_recognition_file, 'wb') as outcsv:\n",
    "        writer = csv.writer(outcsv)\n",
    "        writer.writerow([\"I_est\", \"F\", \"G\", \"A\", \"H\", \"T\"])\n",
    "        \n",
    "    os.remove(RB.db_file)\n",
    "    with open(RB.db_file, 'wb') as outcsv:\n",
    "        writer = csv.writer(outcsv)\n",
    "        writer.writerow([\"name\", \"gender\", \"age\", \"height\", \"times\"])\n",
    "    \n",
    "    analysis_dir = RB.analysis_file.replace(\"/Analysis.json\",\"\")\n",
    "    shutil.rmtree(analysis_dir)\n",
    "    os.makedirs(analysis_dir)\n",
    "        \n",
    "def lbfgs(RB, initial_weights, num_param, recog_folder):\n",
    "\n",
    "    x,f,d = scipy.optimize.fmin_l_bfgs_b(opt_func, initial_weights, args = (RB, recog_folder), approx_grad=1,\n",
    "                                         bounds=[(0.0, 1.0) for i in range(0, num_param)])\n",
    "    print \"weights: \" + str(x)\n",
    "    print \"distance: \" + str(f)\n",
    "    print \"info: \" + str(d)\n",
    "    \n",
    "def opt_func(weights, RB, recog_folder):\n",
    "    print \"*\"*30\n",
    "    if os.path.isfile(RB.recog_file):\n",
    "        resetFiles()\n",
    "        time.sleep(0.01)\n",
    "    print \"weights: \" + str(weights)\n",
    "    identity_list, estimated_probabilities_list = getResults(RB, weights, recog_folder) # get evidence, fill y with 1.0 for the real person, 0.0 for the rest\n",
    "    dist = []\n",
    "    for recog_counter in range(0, len(identity_list)):\n",
    "        x = estimated_probabilities_list[recog_counter]\n",
    "        y = identity_list[recog_counter]\n",
    "        param_ranges[0] = len(y)\n",
    "        dist.append(euclideanDist(x, y, param_ranges[0]))\n",
    "    print \"dist list:\" + str(dist)\n",
    "    max_dist = max(dist)\n",
    "    print \"max distance: \" + str(max_dist)\n",
    "    return max_dist\n",
    "\n",
    "def euclideanDist(x, y, num_people):\n",
    "    # distance_euclidean = sum[(x[i]-y[i])^2], where y[i=real_identity] = 1.0 and y[i!=real_identity] = 0\n",
    "    sum_dist = 0.0\n",
    "    print \"x:\" + str(x)\n",
    "    print \"y:\" + str(y)\n",
    "    for i in range(0, num_people):\n",
    "        sum_dist += math.pow(x[i]-y[i],2)\n",
    "    return sum_dist\n",
    "\n",
    "    \n",
    "def gradientDescent(RB, initial_weights, num_param, param_ranges, dist_function, recog_folder):\n",
    "    #dont start initial weights from 0 or 1\n",
    "    # TODO: the algorithm stops (it is too slow), and the unknown likelihood is low! Check it!!!\n",
    "    start_time = time.time()\n",
    "    gamma = 0.01 # step size multiplier\n",
    "    precision = 0.001\n",
    "    num_iterations = 0\n",
    "    cur_weights = initial_weights\n",
    "    previous_step_size = np.linalg.norm(cur_weights)\n",
    "    \n",
    "    while previous_step_size > precision:\n",
    "        iteration_start_time = time.time()\n",
    "        if os.path.isfile(RB.recog_file):\n",
    "            os.remove(RB.recog_file)\n",
    "            insertEmptyRecognitionCSV()\n",
    "            time.sleep(0.01)\n",
    "        prev_weights = cur_weights[:]\n",
    "        #         evidence_list, likelihood_list, identity_list = getResults(RB, prev_weights, recog_folder) # get evidence and likelihood, fill y with 1.0 for the real person, 0.0 for the rest\n",
    "        evidence_list, bn_list, identity_list, estimated_probabilities_list = getResults(RB, prev_weights, recog_folder) # get evidence, fill y with 1.0 for the real person, 0.0 for the rest\n",
    "\n",
    "        delta_gradient_list = [0 for param in range(0,num_param)]\n",
    "\n",
    "        for recog_counter in range(0, len(evidence_list)):\n",
    "            x = estimated_probabilities_list[recog_counter]\n",
    "            y = identity_list[recog_counter]\n",
    "            evidence = evidence_list[recog_counter]\n",
    "        #             likelihood = likelihood_list[recog_counter]\n",
    "            bn = bn_list[recog_counter]\n",
    "            param_ranges[0] = len(y)\n",
    "            # dx = gradientX(evidence, likelihood, prev_weights, num_param, param_ranges)\n",
    "\n",
    "            dx = gradientXUsingInference(evidence, bn, prev_weights, num_param, param_ranges)\n",
    "            print \"x:\" + str(x)\n",
    "            print \"y:\" + str(y)\n",
    "            print \"dx: \" + str(dx)\n",
    "            if dist_function == \"kullback\":\n",
    "                df = kullbackLeiblerDistGradient(x, y, dx, param_ranges[0], num_param)\n",
    "            else:\n",
    "                df = euclideanDistGradient(x, y, dx, param_ranges[0], num_param)\n",
    "            delta_gradient_list = [delta_gradient_list[count] + gamma * df[count] for count in range(0,num_param)]\n",
    "            print \"delta_gradient_list:\" + str(delta_gradient_list)\n",
    "        delta_gradient_list = [d/float(len(evidence_list)) for d in delta_gradient_list]\n",
    "        cur_weights = np.subtract(prev_weights,delta_gradient_list)\n",
    "        print \"cur_weights:\" + str(cur_weights)\n",
    "        print \"iteration time:\" + str(time.time() - iteration_start_time)\n",
    "        previous_step_size = np.linalg.norm(delta_gradient_list) # norm (cur_weights - prev_weights)\n",
    "        num_iterations += 1\n",
    "    print \"time taken to find optimal weights: \" + str(time.time() - start_time)\n",
    "    print \"num iterations: \" + str(num_iterations)\n",
    "    print \"cur_weights: \" + str(cur_weights)\n",
    "    return cur_weights    \n",
    "\n",
    "def getResults(RB, weights, recog_folder):\n",
    "    isSpanish = False\n",
    "    isDBinCSV = True\n",
    "    \n",
    "    RB.setWeights(weights[0], weights[1], weights[2], weights[3], weights[4])\n",
    "    \n",
    "    recog_csv = recog_folder + \"recognitions.csv\"\n",
    "    \n",
    "    db_file = recog_folder + \"db_to_learn.csv\"\n",
    "    \n",
    "    df = pandas.read_csv(recog_csv, converters={\"F\": ast.literal_eval, \"G\": ast.literal_eval, \"A\": ast.literal_eval, \"H\": ast.literal_eval, \"T\": ast.literal_eval})\n",
    "    recogs_list = df.values.tolist()\n",
    "\n",
    "    df_db = pandas.read_csv(db_file, converters={\"times\": ast.literal_eval})\n",
    "    db_list = df_db.values.tolist()\n",
    "    \n",
    "#     evidence_list = []\n",
    "#     likelihood_list = []\n",
    "#     bn_list = []\n",
    "    identity_list = []\n",
    "    estimated_probabilities_list = []\n",
    "    count_recogs = 0\n",
    "    while count_recogs < len(recogs_list):\n",
    "        print \"p_name: \" + str(recogs_list[count_recogs][0])\n",
    "        isMemoryRobot = True # True if the robot with memory is used (get this from the days maybe?)\n",
    "        isRegistered =  not recogs_list[count_recogs][6]# False if register button is pressed (i.e. if the person starts the session for the first time)\n",
    "        isAddPersonToDB = recogs_list[count_recogs][6] # True ONLY IF THE EXPERIMENTS ARE ALREADY STARTED, THE BN IS ALREADY CREATED, ONE NEW PERSON IS BEING ADDED!FOR ADDING MULTIPLE PEOPLE AT THE SAME TIME, DELETE RecogniserBN.bif FILE INSTEAD!!!\n",
    "        person = []\n",
    "        if isAddPersonToDB:\n",
    "            person = [x for x in db_list if x[0] == recogs_list[count_recogs][0]][0]\n",
    "            isRegistered = False\n",
    "            \n",
    "        # Press either register button (isRegistered = False) or start session button (isRegistered = True)\n",
    "        RB.initSession(isRegistered = isRegistered, isMemoryRobot = isMemoryRobot, isAddPersonToDB = isAddPersonToDB, isDBinCSV = isDBinCSV, personToAdd = person)\n",
    "        # TODO: take a picture and send to robot!\n",
    "        identity_est = RB.startRecognition(recogs_list[count_recogs][1:6]) # get the estimated identity from the recognition network\n",
    "        if RB.num_people > 1:\n",
    "            print \"posterior:\" + str(RB.ie.posterior(RB.I))\n",
    "        print \"identity_est: \" + str(identity_est)\n",
    "        \n",
    "#         evidence_list.append(RB.getNonweightedEvidenceResult())\n",
    "        \n",
    "#         likelihood_list_cur = []\n",
    "#         for name in RB.node_names[1:]:\n",
    "#             likelihood_list_param = []\n",
    "#             id_v = RB.r_bn.idFromName(name)\n",
    "#             for p in RB.i_labels:\n",
    "#                 likelihood_list_param.append(RB.r_bn.cpt(id_v)[{'I':p}][:])\n",
    "#             likelihood_list_cur.append(likelihood_list_param)\n",
    "                \n",
    "#         likelihood_list.append(likelihood_list_cur)\n",
    "\n",
    "#         bn_list.append(RB.r_bn)\n",
    "    \n",
    "        identity_list_cur = [0 for i in range(0, len(RB.i_labels))]\n",
    "        if recogs_list[count_recogs][0] in RB.i_labels:\n",
    "            identity_list_cur[RB.i_labels.index(recogs_list[count_recogs][0])] = 1\n",
    "        else:\n",
    "            identity_list_cur[RB.i_labels.index(RB.unknown_var)] = 1\n",
    "        identity_list.append(identity_list_cur)\n",
    "        \n",
    "        estimated_probabilities_list.append(RB.getEstimatedProbabilities())\n",
    "        \n",
    "        p_name = None\n",
    "        isRecognitionCorrect = False\n",
    "        if isMemoryRobot:\n",
    "            if isRegistered:\n",
    "                if identity_est != \"unknown\":\n",
    "                    # TODO: ask for confirmation of identity_est on the tablet (isRecognitionCorrect = True if confirmed) \n",
    "                    if identity_est == recogs_list[count_recogs][0]:\n",
    "                        isRecognitionCorrect = True # True if the name is confirmed by the patient\n",
    "        \n",
    "        if isRecognitionCorrect:\n",
    "            RB.confirmPersonIdentity(recog_results_from_file = recogs_list[count_recogs][1:6]) # save the network, analysis data, csv for learning and picture of the person in the tablet\n",
    "        else:\n",
    "            if isAddPersonToDB:\n",
    "                p_name = person[0]\n",
    "                count_recogs += 1\n",
    "                RB.confirmPersonIdentity(p_name = p_name, recog_results_from_file = recogs_list[count_recogs][1:6])\n",
    "                \n",
    "#                 evidence_list.append(RB.getNonweightedEvidenceResult())\n",
    "#                 likelihood_list_cur = []\n",
    "#                 for name in RB.node_names[1:]:\n",
    "#                     likelihood_list_param = []\n",
    "#                     id_v = RB.r_bn.idFromName(name)\n",
    "#                     for p in RB.i_labels:\n",
    "#                         likelihood_list_param.append(RB.r_bn.cpt(id_v)[{'I':p}][:])\n",
    "#                     likelihood_list_cur.append(likelihood_list_param)\n",
    "\n",
    "#                 likelihood_list.append(likelihood_list_cur)\n",
    "\n",
    "#                 bn_list.append(RB.r_bn)\n",
    "\n",
    "                identity_list_cur = [0 for i in range(0, len(RB.i_labels))]\n",
    "                identity_list_cur[RB.i_labels.index(p_name)] = 1\n",
    "                identity_list.append(identity_list_cur)\n",
    "                \n",
    "                estimated_probabilities_list.append(RB.getEstimatedProbabilities())\n",
    "                \n",
    "                if RB.num_people > 1:\n",
    "                    print \"posterior:\" + str(RB.ie.posterior(RB.I))\n",
    "                \n",
    "            else:\n",
    "                p_name = recogs_list[count_recogs][0] # TODO: ask for patient name (p_name) on tablet\n",
    "                RB.confirmPersonIdentity(p_name = p_name, recog_results_from_file = recogs_list[count_recogs][1:6])\n",
    "        print \"-\"*10\n",
    "        count_recogs += 1\n",
    "    \n",
    "#     return evidence_list, likelihood_list, identity_list, estimated_probabilities_list\n",
    "#     return evidence_list, bn_list, identity_list, estimated_probabilities_list\n",
    "    return identity_list, estimated_probabilities_list\n",
    "\n",
    "\n",
    "def euclideanDistGradient(x, y, dx, num_people, num_param):\n",
    "    # distance_euclidean = sum[(x[i]-y[i])^2], where y[i=real_identity] = 1.0 and y[i!=real_identity] = 0\n",
    "    sum_dist = [0 for count in range(0, num_param)]\n",
    "    for param in range(0, num_param):\n",
    "        for i in range(0, num_people):\n",
    "            sum_dist[param] += 2*(x[i]-y[i])*dx[param][i]\n",
    "    return sum_dist\n",
    "\n",
    "def kullbackLeiblerDistGradient(x, y, dx, num_people, num_param):\n",
    "    # distance_euclidean = sum[(x[i]-y[i])^2], where y[i=real_identity] = 1.0 and y[i!=real_identity] = 0\n",
    "    sum_dist = [0 for count in range(0, num_param)]\n",
    "    for param in range(0, num_param):\n",
    "        for i in range(0, num_people):\n",
    "            if y[i] != 0:\n",
    "                sum_dist[param] += y[i]*dx[param][i]/float(x[i])\n",
    "    return sum_dist\n",
    "\"\"\"\n",
    "def gradientX(evidence, likelihood, weights, num_param, param_ranges):  \n",
    "    dx = []\n",
    "    print \"param_ranges:\" + str(param_ranges)\n",
    "    for param in range(0, num_param):\n",
    "        sum_posterior_total = 0\n",
    "        sum_posterior = [0 for count in range(0, param_ranges[0])]\n",
    "        for i in range(0, param_ranges[0]):\n",
    "            for f in range(0, param_ranges[0]):\n",
    "                for g in range(0, param_ranges[1]):\n",
    "                    for a in range(0, param_ranges[2]):\n",
    "                        for h in range(0, param_ranges[3]):\n",
    "                            for t in range(0, param_ranges[4]):\n",
    "                                mult_likelihood = 1.0\n",
    "                                for k in range(0, num_param):\n",
    "                                    if k == 0:\n",
    "                                        index_param = f\n",
    "                                    elif k == 1:\n",
    "                                        index_param = g\n",
    "                                    elif k == 2:\n",
    "                                        index_param = a\n",
    "                                    elif k == 3:\n",
    "                                        index_param = h\n",
    "                                    elif k == 4:\n",
    "                                        index_param = t\n",
    "                                    if k == param:\n",
    "                                        mult_likelihood *= derivativeLikelihood(evidence[k], weights[k], index_param)*likelihood[k][i][index_param]\n",
    "                                    else:\n",
    "                                        mult_likelihood *= normEvidence(evidence[k], weights[k], index_param)*likelihood[k][i][index_param]\n",
    "                                sum_posterior[i] += mult_likelihood\n",
    "            sum_posterior_total += sum_posterior[i]\n",
    "        dx.append([p/sum_posterior_total for p in sum_posterior])\n",
    "    return dx\n",
    "\"\"\"  \n",
    "\n",
    "def gradientXUsingInference(evidence, bn, weights, num_param, param_ranges):\n",
    "    calculated_evidence = []\n",
    "    dx = []\n",
    "    for param in range(0, num_param):\n",
    "        sum_weighted = sumWeightedEvidence(evidence[param], weights[param])\n",
    "        log_sum_weighted = logSumWeightedEvidence(evidence[param], weights[param])\n",
    "        calc_ev = []\n",
    "        for p in range(0, param_ranges[param]):\n",
    "            calc_ev.append(derivativeLikelihood(sum_weighted, log_sum_weighted, evidence[param], weights[param], p))\n",
    "        calculated_evidence.append(calc_ev)\n",
    "    for param in range(0, num_param):\n",
    "        evidence_to_set = evidence[:]\n",
    "        evidence_to_set[param] = calculated_evidence[param]\n",
    "        post_i = RB.getPosteriorIUsingCalculatedEvidence(bn, evidence_to_set)\n",
    "        dx.append(post_i)\n",
    "    return dx\n",
    "\n",
    "def sumWeightedEvidence(evidence, weight):\n",
    "    sum_weighted = 0.0\n",
    "    for count in range(0, len(evidence)):\n",
    "        sum_weighted += math.pow(evidence[count], weight)\n",
    "    return sum_weighted\n",
    "\n",
    "def logSumWeightedEvidence(evidence, weight):\n",
    "    log_sum_weighted = 0.0\n",
    "    for count in range(0, len(evidence)):\n",
    "        log_sum_weighted += math.log(evidence[count])*math.pow(evidence[count], weight)\n",
    "    return log_sum_weighted\n",
    "\n",
    "def derivativeLikelihood(sum_weighted, log_sum_weighted, evidence, weight, index_param):       \n",
    "    return math.pow(evidence[index_param],weight)*(math.log(evidence[index_param])*sum_weighted - log_sum_weighted)/(sum_weighted*sum_weighted)\n",
    "                            \n",
    "def normEvidence(evidence, weight, index_param):\n",
    "    sum_weighted = 0.0\n",
    "    for count in range(0, len(evidence)):\n",
    "        sum_weighted += math.pow(evidence[count], weight)\n",
    "    return math.pow(evidence[index_param],weight)/float(sum_weighted)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    start_time = time.time()\n",
    "    RB = RM.RecogniserBN()\n",
    "    num_param = 5 # F,G,A,H,T\n",
    "    initial_weights = [1.0, 0.6, 0.4, 0.2, 0.6]\n",
    "    param_ranges = [1, len(RB.g_labels), RB.age_max-RB.age_min+1, RB.height_max-RB.height_min+1, RB.time_max-RB.time_min+1]\n",
    "    dist_function = \"euclidean\"\n",
    "    recog_folder = \"TestCases/TestCase6/\"\n",
    "    RB.setFilePaths(recog_folder)\n",
    "#     gradientDescent(RB, initial_weights, num_param, param_ranges, dist_function, recog_folder)\n",
    "    lbfgs(RB, initial_weights, num_param, recog_folder)\n",
    "    print \"total time:\" + str(time.time() - start_time)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
